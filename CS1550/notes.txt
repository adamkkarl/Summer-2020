5/21/2020
CPU
1. What is a process?
2. How does context switching happen?
3. How does scheduling happen?
4. Bugs due to processes sharing a CPU

process - a program that's running
  has memory files, registers, process id (pid)

Process Control Block (PCB)
  (linux: task control block)
  pointer to process memory
  process state (running, waiting, etc)
  program counter and CPU registers
  CPU scheduling info
  memory management info
  accounting info (CPU used, clock time elapsed, etc)
  I/O status info

Context switching
1. CPU stores registers in old PCB
2. CPU reads registers from new PCB
  instructions ~1e-9s  mem access ~1e-6s
  so context switching has significant overhead

Scheduling
process state - is processs ready to run? waiting for network packet?
new, ready (in queue), running, waiting (on I/O or event), terminated

Schedulers
Short-term scheduler (CPU scheduler)
  selects next process and allocates CPU
  invoked frequently (ms) - must be fast
Long-term scheduler
  selects process to bring to queue
  infrequent (sec/min) - can be slow
  controls degree of multiprograming (processes in memory)
Medium-term scheduling
  when too many processes fill memory
  *swap* some out to disk temporarily

Queueing diagram
-> ready queue -> CPU ->
  -> terminate
  -> I/O request -> I/O queue -> I/O -> ready queue
  -> time slice expires -> ready queue
  -> fork a child -> child executes -> ready queue
  -> wait for interrupt -> interrupt occur -> ready queue

5/26/2020
fork - system call that creates a process
child gets new PCB
  knows pid of parent and copy of all parent's registers
child gets copy of parent's memory

fork() writes eax register to return a value
  parent: fork() returns pid of child
  child: fork() returns 0

ret = fork();
if (ret>0) {...parent code...}
if (ret==0){...child code...}

copy_on_write - setting to avoid overhead from forking
  only copies parent memory to child process memory if necessary
  NO memory copy on fork
  useful since often child process is completely different
    (ex: command line -> java runtime)

Process Creation
parent processes create children processes, creating tree
process identifier (pid) - identifies and manages processes
Resource sharing options:
  -parent and child chare all resources
  -children share subset (code, data, heap) of parent resources
  -parent and child share no resorces
Execution options
  -parent and child concurrent
  -parent wait()'s until children terminate

busy waiting/spinning - executing a long loop of nothing
  for(int i=0; i<10000000000; i++)

Orphan Process
  child with terminated parent
  gets adopted by OS process (id=1)

Zombie Process
  terminated process with parent still running
  process stays alive until parent wait()'s for it
  Bad - will take up memory until parent collects or orphans it

Process Termination
  parent can abort() a child if needed

5/28/2020
ThreadTest.java
for(int i=0; i<3; i++) {
  pthread_create(&tid[i], &attr, runner, (void *)((long long) i));
}
  //thread - children share heap/code/global vars of parent

void *runner(void *param) {
  int i = ((int)param);
  printf("%d param = %d\n", (int)tid[i], i);
  sum[i] = 0;
  for (int j=1; j<=n; j++)
    sum[i] += j;
  pthread_exit(0);
}

Multiprocessing Bugs

Bounded Buffer Problem
  Circle Queue
  Producer process - adds to queue, increments size
  Consumer process - removes from queue, decrements size
Problem: Race Conditions
updating memory is not atomic (uninterruptable)

When it works
x==3
P1	P2
R1<=x		load
R1=R1+1		change
R1=>x		store
	R3<=x	load
	R3=R3+1	change
	R3=>x	store
x==5

When it gets interrupted (BUG)
x==3
P1	P2
R1<=x		load
	R3<=x	load
	R3=R3+1	change
R1=R1+1		change
R1=>x		store
	R1=>x	store
x==4

1 thread at a time:
private synchronized void addSum (Integer sum)
  sums.add(sum);

Critical Regions
  used to procide *mutual exclusion* and help fix race conditions
4 conditions for mutual exclusion
  1. No 2 processes simultaneously in critical region
  2. No assumptions about the speeds or number of CPUs
  3. No process outside of critical region can block another process
  4. No process must wait forever to enter its critical region

Busy Waiting: Strict Alternation
Use shared variable (turn) to keep track of whose turn
Continuously spins reading variable until it sees it can procede (spin lock)
Avoids race conditions, but fails #3 if critical region finishes, but interrupts before changes turn value

*Doesn't work with modern out-of-order compilers, might increment turn before critical region executes

Busy Waiting: Working Solution
void enter_region(int process) {
  int turn; //whose turn is it
  int interested[N]; //set to TRUE if process j is interested
  while (turn==other && interested[other]==TRUE) //solves #3 criteria
    ;
{

void leave_region(int process)
  interested[process] = FALSE;

Hardware for Synchronization
software sync works, but may be complex and busy waiting wastes CPU time
methods:
  Test and Sit - test variable and set it in 1 clock cycle
  Atomic Swap - swap register and memory in 1 clock cycle
  Turn off interrupts - disable interrupts during critical section
    dangerous: can hijack CPU

int lock = 0; //lock open

TEST AND SET
while (TestAndSet(lock))
  ;
//critical section
lock = 0
//noncritical section

SWAP
while(Swap(lock, 1) == 1)
  ;
//critical section
lock = 0;
//noncritical section

Basically if lock==0, first come first serve for critical section
Possible requirements issue -> can have unbounded waiting

Eliminating Busy Waiting
both harware and software solutions require wasteful spin locks

spin locking efficient if:
  1. multi-core CPU (might unlock before process gets rotated out)
  2. short critical section
  3. few contending processes

Priority Inversion
when a high priority process has to wait on a low priority process
solution: *semaphores*
Down(S): while (S<=0) {}; S-=1;  //also called P()
Up(S): S+=1;			 //also called V()

Shared variable: Semaphore mutex; //initialize to 1
	Pi
while(1) {
  down(mutex);
  //CS
  Up(mutex);
  //noncrit
}

SEMAPHORE CODE
class Semaphore {
  int val;
  ProcessList p1;
  void down();
  void up();
};

Semaphore::down() {
  //lock()
  val-=1
  if(val<0)
    //add process to p1
    //unlock()
    sleep()
}

Semaphore::up() {
  Process p;
  //lock()
  val += 1;
  if (val<=0)
    //remove process p from p1
  wakeup(p)
  //unlock()
}

Semaphore Metaphor
line of people to write on board, 3 markers (S=3)
or
red/green light at train junction where 1 gets through at a time (S=1)

6/2
Critical Section Problem Approaches
  1. Taking turns - user level software
  2. Atomic hardware instructions - spin locking
  3. Semaphores - needs OS support

Process calls down/wait/P on semaphore S to enter critical section
  -may block
  -may continue executing
  -may become ready for execution
  -requires OS support to take over and carry out system call

Semaphores for synchronization
Goal: execute B on P1 after A on P0

shared vars
Semaphore flag = 0; //flag starts down

P0
//execute A
flag.up()

P1
flag.down() //cannot happen until A puts flag up
//execute B

Types of Semaphores
Binary Semaphores S=0 or 1
Counting Semaphores S=2+

Deadlock and Starvation
Deadlock - 2+ processes waiting for another waiting process
  P0 has A, needs B
  P1 has B, needs A
  each waiting for the other
  might only happen extremely infrequently when context switches on exact line

Starvation - system making process, but low prio process waits indefinitely since it keeps getting jumped in line

MONITORS
monitor - another high-level sync primitive
  -enforces mutual exlusion
  -only 1 process in monitor at a time
  -implemented by the language/compiler, can also be implemented with semaphores
  -Java: public synchronized class MyClass {...}
    -all Java Objects (string, int, etc) are monitors (toString(), equals(), wait(), notify() are critical)

P0
myMon.proc1();
P1
myMon.proc2();
one will execute, other cannot start until first is done

How can a process wait inside a monitor?
  -cannot sleep, would block others from entering
  -Soln: use condition variable

Condition Variables: support 2 ops
  wait(): suspend process until signaled
  signal(): wake up 1 process waiting on condition variable (if any)
condition variables are only usable withing monitors

When signaling process P0 signals waiting process P1
  -Mesa semantics: P0 continues running
  -Hoare semantics: P1 runs within monitor
    can be implemented with semaphores
    useful when condition P1 waited on might not still be true once P0 leaves monitor

P0: m.f1();
P1: m.f2();
Monitor
int x=0;
Cv cv;
f1() {x=1; cv.signal(); x=0;}
f2() {while(x==0) cv.wait(); ...(rest assumes x!=0)
  must use while loop in Mesa semantics, otherwise wakes when x=1 but runs when x=0

Locks and Condition Variables
monitors require native language support
  -locks need acquire(), release() to enter/exit monitor
  -CVs need wait(), signal()

Condition Variable Usage
  each CV is associated with 1 lock
  lock must be helpd to use CV
  waiting on CV releases lock implicitly
  returning from wait() on CV reacquires the lock

Implementing lock using Semaphores (but not CV support)
  Hoare semantics (Mesa needs wait() syscall)

class Lock {
  Semaphore mutex(1); //lock open (1), closed (0)
  Semaphore next(0);
  int nextCount = 0;
};

Lock::Acquire() {mutex.down();}

Lock::Release() {
  if(nextCount > 0) //if something's waiting on lock
    next.up() //give lock to next
  else
    mutex.up() //open lock
}

every method in monitor needs to have
f1() {
  l.acquire();
  ...
  l.release();
}


Implementing Condition Variables

class Condition {
  Lock *lock;
  Semaphore condSem(0);
  int semCount = 0; //number waiting on condition variable (in monitor)
}

Condition::Wait() {
  semCount+=1;
  if(lock->nextCount > 0)
    lock->nextUp(); //wake process from next queue (in monitor)
  else
    lock->next.up(); //if none, wake process waiting FOR monitor
  condSem.down(); //send self to sleep
  semCount-=1;
}

Condition::Signal() {
  if(semCount > 0) {
    lock

6/4
Message Passing - useful for UI thread with other threads sending a message of
what they want changed
2 primitives: send and receive
Issue: how does sender know message got sent?

Barrier
  -processes wait at barrier until all in group arrive
  -once all arrive, all can proceed
  -may be implemented using locks and condition variables

CRITICAL SECTION SOLUTIONS
  -taking turns (software)
  -atomic hardware instructions (spinlock)
  -semaphores (sleeplock)
  -monitors and condition variables
  -message passing

Classical Synchronization Problems - see cs1550 page for solutions
Bounded buffer: circular array queue with producers and consumers
Readers & Writers: if one process is writing, no others can access shared variable
Dining Philosophers: need adjacent chopsticks to eat, then release and think
Sleepy Barber: finite queue, single barber


Quiz 6/6
The operating system continuously runs on the CPU and is always resident in memory: FALSE
With modern hardware and out-of-order execution with multipple cores, it's possible to solve Critical Section problems with software alone: FALSE
Assuming lock is a shared int, starting with lock==0, if 2 processes concurrently execute TestAndSet(lock) they will both get 1 as a return val: FALSE
A shared int that's used to implement a spinlock using TestAndSet should be initialized to 0,whereas a semaphore used to implement sleeplock should be initialized to 1: TRUE
When the following runs, fork will be called a total of 4 times:
  int main() {fork(); fork(); fork(); fork();} : FALSE
Copy-on-Write is a mechanism that avoids unneccessary memory copying when fork() is immediately followed by exec() in the child process: TRUE
All processes createdd using fork() have a process ID of 0: FALSE
Zombie processes waste system resources because the operating system does not free up their resources until their parents wait() on them: TRUE
On a single-core system, since the CPU can do only one thing at a time, there is no need to protect shared data in a multi-process application: FALSE
The code below is free from race conditions since every access to the shared variable x is surrounded by a lock/unlock pair
  int x=0;
  Lock l;
  l.acquire();
  if (x==0) {
    l.release();
    l.acquire();
    x++;
  }
  l.release();
  FALSE **
Each running thread has its own register set and stack: TRUE
A CPU-bounded process may not issue any input/output operations: FALSE
Threads are cheaper to context switch than processes: TRUE
In a single-core system, it is safe to assume that mutual exclusion is guaranteed when interrupts are disabled around a critical section: FALSE **
The stack of a thread can be corrupted by a thread of another process: FALSE
System calls change the privelege mode of the processor: TRUE
Microkernel operating systems are faster then monolithic systems: FALSE
Threads are cheaper to create than processes: TRUE
Consider parent Process P forks child process C. When P terminates, C becomes a zombie: FALSE
Which is not part of the OS: interrupt service routines, system call implementations, interrupt descriptor table, system call table, INTERRUPT DESCRIPTOR TABLE REGISTER


6/9
Semaphores can represent:
  Multi-instance Resource
    grab resource: down
    put resource back: up
  Waiting area (with n "seats")
    entering waiting area: down
    leaving waiting area: up
  Mutex
    (when S=0 or S=1 only)
(rest of class was monitor solutions to classical sync problems)

6/11
Resources
  preemtable resources: can be taken away with no ill effects
  nonpreemptable resources: can cause process to fail if taken away

4 conditions for deadlock
  mutual exclusion: each resource assigned to at most 1 process
  hold & wait: a process holding resources can request more resources
  no preemption: cannot forcibly take away resources
  circular wait: chain of 2+ processes each waiting resource held by next process

The Ostrich Algorithm (pretend there's no deadlock possible)
  reasonable if: deadlocks rare; cost of prevention high
  UNIX and windows take this approach
  trade off between convenience and correctness

Detecting Deadlocks using graphs
  find cycle in resource allocation graph (means deadlock exists iff 1 of each resource)
  better detection alg uses 2D array of which processes have which resources


Quiz 6/15
Every semaphore initialized to 1 is a binary semaphore: FALSE
Which of the following may occur when a process calls broadcast on a condition variable?  more than one process wake up but only one of them immediately runs; no process wakes up; one process wakes up and immediately runs

A job is in the ready queue if it: has all resources needed to execute except the CPU
When a process is created using the classical fork() system call, which of the following is not inherited by the child process? process ID
Consider a system with two or more processes that run concurrently and share some variables. If errors in shared variables may result depending on the exact interleaving of the processes' instructions, the system has: race condition
(Mark all that apply) Which of the following operations require(s) the executing code to be operating with high privilege (i.e., in kernel mode)? disabling interrupts, setting the system clock


6/16
Recovery from deadlock
  preemption: take resources forcibly from other processes
  rollback: checkpoint a process periodically, may reset later
  kill process: crude but simple, preferrably pick process that hasn't run far

aside: let process holding semaphore run with same prio as highest process
waiting for that semaphore (Mars rover example)

Banker's Algorithm for preventing deadlocks
upon resource request, pretend request is granted, then check for deadlock
must know:
  resources allocated to each process (2D matrix)
  maximum of each resource each process would need to finish (2D matrix)
  total available of each resource

Preventing Deadlock
Ensure at least 1 of the conditions for deadlock never occurs
  -mutual exclusion
  -hold & wait
  -circular wait
  -no preemption

mutual exclusion: some devices (like printer) can be *spooled*
  spooled: separate process queues tasks
  benefit: as few processes as possible ever hold the resource

hold & wait: ask for all resources before running process
  downsides: don't always know what you'll need
  variation: give up all resources before new request

circular wait: assign numbers to resources, always request in numerical order
  (no way to complete a cycle if implemented properly)

6/18
User-oriented criteria
  Response time: time between submission of request and receipt of request
  Turnaround time: time from submission to completion
System-oriented criteria
  Processor utilization
  Throughput: number of processors completed per unit time
  Fairness

Classification
  Long-term: which process to admit
  Medium-term: which process to swap in/out
  Short-term: which ready process to execute next

CPU I/O cycle - CPU burst followed by I/O burst

Scheduling algorithms
  First-come, first served (FCFS)
  Shortest Job First (or Shortest Process Next)
  Priority
  Round-Robin
  Multilevel Queue
  Multilevel Feedback Queue

selection function - determines which ready process is next for execution
decision mode - specifies the instants in time the selection function is exercised
  Nonpreemptive - run until it blocks for I/O (or terminates)
  Preemptive - clock interrupt for context switch

service time = processor time for one CPU-I/O cycle
  long service time - CPU-bound, or "long jobs"
Avg wait time = avg turnaround time - avg service time

FCFS
  non-preemptive, processes run in order they arrive until I/O request
  favors CPU-bound processes
  better I/O util would be possible if I/O-bound had higher priority
  Convoy effect - happens when process with long CPU burst occupies the CPU

Shortest Job First
  non-preemptive, process with shortest expected CPU burst time runs
  CPU burst time must be estimated for each process

6/23
Shortest Job First Critique
  possibility of starving long processes
  preemption is mandatory for time-sharing environment
  gives implicit priority to short jobs

SJF optimizes avg turnaround time EXCEPT when processes may arrive at any time

Priorities Scheduling - explicit user assigned priorities
  multiple ready queues for each prio level
  low prio = possible starvation
  dynamic priorities = can increase based on age or execution history

Round-Robin: for high responsiveness, bad turnaround
  selection: FCFS
  decision mode: preemptive
  processor runs until time-slice "quantum" expires (usually 10-100ms)
Time quantum
  must be longer than clock interrupt + dispatching overhead
  should be longer than typical interaction (CPU burst)

Multilevel Feedback Scheduling
  preemptive scheduling with dynamic properties
  each priority has queue with its own scheduling algorithm
  new process starts in RQ0, moves down after each quantum w/o I/O call
    I/O-bound stay high prio, CPU-bound drift down
  often 1 priority lower = double quantum length
  long jobs may starve

Algorithm comparison
best choice depends on:
  system workload
  hardware support for dispatcher
  relative importance of criteria (responce time, CPU util, throughput)
  


Quiz 6/30
The waiting time of a process is the finish time - the arrival time: FALSE
The two decision modes of CPU scheduling algorithms are: preemptive and non-preemptive
Preemption is done by the kernel: after the quantum expires
Multi-level queues scheduling with ageing: is a fair scheduling policy
A CPU scheduler that assigns higher prio to the I/O-bound processes than the CPU-bound processes causes: high CPU util; high I/O util
A CPU scheduler that assigns higher prio to the CPU-bound processes than the I/O-bound processes causes: high CPU util; low I/O util
A disk scheduler that assigns higher prio to the I/O-bound processes than the CPU-bound processes causes: (low; high)
A disk scheduler that assigns higher prio to the I/O-bound processes than the CPU-bound processes causes: (high; high)
A scheduler that prioritizes I/O-bound processes usually does not significantly delay the completion of CPU-bound processes: TRUE
Which of the following is true of non-preemptive scheduling: a process keeps the CPU until it terminates or switches to the waiting state
Which technique is the fastest to track changes in the CPU burst length of a process? exponential averaging with alpha = 0.8
FCFS scheduling is inherently starvation-free: TRUE

Quiz 7/3
FCFS favors I/O bound processes, SJF favors CPU-bound: FALSE
Deadlock detection is a fast operation and is thus adopted by almost all gen-purpose OS's: FALSE
When using resource ordering, cycles never occur in the resource-allocation graph: TRUE
Assume 1ms context switch, 100ms timeslice, avg of 1ms process runtime. What's the avg CPU util? less than 50%
Which of the following is true in a system that detects deadlocks? the OS has to know the max number of resources for each process; deadlocks may occur but will eventually be detected
Which of the following deadlock conditions is directly prevented by spooling? mutual exclusion
spurious wakeup
